<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="referrer" content="no-referrer">
    
    <meta property="og:title" content="Inference of TensorFlow model without TensorFlow" />
    <meta property="og:locale" content="en" />
    <meta property="og:description" content="Training of machine learning models is fun, but itâ€™s a useless waste of energy if you are not going to use them afterwards. Usage patterns can be very" />
    <meta property="og:url" content="https://intmaker.com/2019/tensorflow-inference/" />
    <meta property="og:site_name" content="Xpiks" />
    
    
      <link href='https://fonts.googleapis.com/css?family=Open+Sans:400|Old+Standard+TT:400' rel='stylesheet' type='text/css'>
      <link href="https://fonts.googleapis.com/css?family=Cookie&display=swap" rel="stylesheet">
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css">
    

    <link rel="icon" type="image/png" href="https://intmaker.com/favicon_16x16.png" sizes="16x16">
    <link rel="icon" type="image/png" href="https://intmaker.com/favicon_32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://intmaker.com/favicon_128x128.png" sizes="128x128">

    <title>
      
      
         Inference of TensorFlow model without TensorFlow 
      
    </title>
    <link rel="canonical" href="https://intmaker.com/2019/tensorflow-inference/">

    <style>
  * {
    border:0;
    font:inherit;
    font-size:100%;
    vertical-align:baseline;
    margin:0;
    padding:0;
    color: black;
    text-decoration-skip: ink;
  }

  body {
    font-family:'Open Sans', 'Myriad Pro', Myriad, sans-serif;
    font-size:17px;
    line-height:160%;
    color:#1d1313;
    max-width:700px;
    margin:auto;
  }

  p {
    margin: 20px 0;
  }

  a img {
    border:none;
  }

  img {
    margin: 10px auto 10px auto;
    max-width: 100%;
    display: block;
  }

  .left-justify {
    float: left;
  }

  .right-justify {
    float:right;
  }

  pre, code {
    font: 12px Consolas, "Liberation Mono", Menlo, Courier, monospace;
    background-color: #f7f7f7;
  }

  code {
    font-size: 12px;
    padding: 4px;
  }

  pre {
    margin-top: 0;
    margin-bottom: 16px;
    word-wrap: normal;
    padding: 16px;
    overflow: auto;
    font-size: 85%;
    line-height: 1.45;
  }

  pre>code {
    padding: 0;
    margin: 0;
    font-size: 100%;
    word-break: normal;
    white-space: pre;
    background: transparent;
    border: 0;
  }

  pre code {
    display: inline;
    max-width: auto;
    padding: 0;
    margin: 0;
    overflow: visible;
    line-height: inherit;
    word-wrap: normal;
    background-color: transparent;
    border: 0;
  }

  pre code::before,
  pre code::after {
    content: normal;
  }

  em,q,em,dfn {
    font-style:italic;
  }

  .sans,html .gist .gist-file .gist-meta {
    font-family:"Open Sans","Myriad Pro",Myriad,sans-serif;
  }

  .mono,pre,code,tt,p code,li code {
    font-family:Menlo,Monaco,"Andale Mono","lucida console","Courier New",monospace;
  }

  .heading,.serif,h1,h2,h3 {
    font-family:"Old Standard TT",serif;
  }

  strong {
    font-weight:600;
  }

  q:before {
    content:"\201C";
  }

  q:after {
    content:"\201D";
  }

  del,s {
    text-decoration:line-through;
  }

  blockquote {
    font-family:"Old Standard TT",serif;
    text-align:center;
    padding:50px;
  }

  blockquote p {
    display:inline-block;
    font-style:italic;
  }

  blockquote:before,blockquote:after {
    font-family:"Old Standard TT",serif;
    content:'\201C';
    font-size:35px;
    color:#403c3b;
  }

  blockquote:after {
    content:'\201D';
  }

  hr {
    width:40%;
    height: 1px;
    background:#403c3b;
    margin: 25px auto;
  }

  h1 {
    font-size:35px;
  }

  h2 {
    font-size:28px;
  }

  h3 {
    font-size:22px;
    margin-top:18px;
  }

  h1 a,h2 a,h3 a {
    text-decoration:none;
  }

  h1,h2 {
    margin-top:28px;
  }

  #sub-header, time {
    color:#403c3b;
    font-size:13px;
  }

  #sub-header {
    margin: 0 4px;
  }

  #nav h1 a {
    font-size:35px;
    color:#1d1313;
    line-height:120%;
  }

  .posts_listing a,#nav a {
    text-decoration: none;
  }

  li {
    margin-left: 20px;
  }

  ul li {
    margin-left: 5px;
  }

  ul li {
    list-style-type: none;
  }
  ul li:before {
    content:"\00BB \0020";
  }

  #nav ul li:before, .posts_listing li:before {
    content:'';
    margin-right:0;
  }

  #content {
    text-align:left;
    width:100%;
    font-size:15px;
    padding:60px 0 80px;
  }

  #content h1,#content h2 {
    margin-bottom:5px;
  }

  #content h2 {
    font-size:25px;
  }

  #content .entry-content {
    margin-top:15px;
  }

  #content time {
    margin-left:3px;
  }

  #content h1 {
    font-size:30px;
  }

  .highlight {
    margin: 10px 0;
  }

  .posts_listing {
    margin:0 0 50px;
  }

  .posts_listing li {
    margin:0 0 25px 15px;
  }

  .posts_listing li a:hover,#nav a:hover {
    text-decoration: underline;
  }

  #nav {
    text-align:center;
    position:static;
    margin-top:60px;
  }

  #nav ul {
    display: table;
    margin: 8px auto 0 auto;
  }

  #nav li {
    list-style-type:none;
    display:table-cell;
    font-size:15px;
    padding: 0 20px;
  }

  #links {
    margin: 50px 0 0 0;
  }

  #links :nth-child(2) {
    float:right;
  }

  #not-found {
    text-align: center;
  }

  #not-found a {
    font-family:"Old Standard TT",serif;
    font-size: 200px;
    text-decoration: none;
    display: inline-block;
    padding-top: 225px;
  }

  @media (max-width: 750px) {
    body {
      padding-left:20px;
      padding-right:20px;
    }

    #nav h1 a {
      font-size:28px;
    }

    #nav li {
      font-size:13px;
      padding: 0 15px;
    }

    #content {
      margin-top:0;
      padding-top:50px;
      font-size:14px;
    }

    #content h1 {
      font-size:25px;
    }

    #content h2 {
      font-size:22px;
    }

    .posts_listing li div {
      font-size:12px;
    }
  }

  @media (max-width: 400px) {
    body {
      padding-left:20px;
      padding-right:20px;
    }

    #nav h1 a {
      font-size:22px;
    }

    #nav li {
      font-size:12px;
      padding: 0 10px;
    }

    #content {
      margin-top:0;
      padding-top:20px;
      font-size:12px;
    }

    #content h1 {
      font-size:20px;
    }

    #content h2 {
      font-size:18px;
    }

    .posts_listing li div{
      font-size:12px;
    }
  }

  @media (prefers-color-scheme: dark) {
    *, #nav h1 a {
      color: #FDFDFD;
    }

    body {
      background: #121212;
    }

    pre, code {
      background-color: #262626;
    }

    #sub-header, time {
      color: #BABABA;
    }

    hr {
      background: #EBEBEB;
    }
  }

  .decorrated .upped {
      display: none;
  }

  .decorrated:after {
    content: attr(data-n) "@" attr(data-d) "." attr(data-t); 
  }

  .sharing-icons {
      padding-left: 10px;
      margin-bottom: 20px;
  }

  section.disqus {
      margin-top: 100px;
  }

  .social-icons {
      float: right;
  }

  .social-icons a {
      font-size: 20px;
      margin-left: 10px;
  }

  @media (max-width: 30em) {
      .social-icons {
          float: left;
          padding-top: 0px;
      }
  }

  .bmc-button img {
      width: 27px !important;
      margin: 0 0 10px 0 !important;
      box-shadow: none !important;
      border: none !important;
      vertical-align: middle !important;
      display: inline !important;
  }

  .bmc-button {
      line-height: 36px !important;
      height:37px !important;
      text-decoration: none !important;
      display:inline-block !important;
      color:#000000 !important;
      background-color:#FFFFFF !important;
      border-radius: 3px !important;
      border: 1px solid transparent !important;
      padding: 1px 9px !important;
      font-size: 23px !important;
      letter-spacing: 0.6px !important;
      box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;
      -webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;
      margin: 0 auto !important;
      font-family:'Cookie', cursive !important;
      -webkit-box-sizing: border-box !important;
      box-sizing: border-box !important;
      -o-transition: 0.3s all linear !important;
      -webkit-transition: 0.3s all linear !important;
      -moz-transition: 0.3s all linear !important;
      -ms-transition: 0.3s all linear !important;
      transition: 0.3s all linear !important;
  }

  .bmc-button:hover, .bmc-button:active, .bmc-button:focus {
      -webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;
      text-decoration: none !important;
      box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;
      opacity: 0.85 !important;
      color:#000000 !important;
  }

</style>


    

    <script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');  ga('create', 'UA-10412407-14', 'auto');  ga('send', 'pageview');  ga('set', 'anonymizeIp', true); </script>
  </head>

  <body>
    <section id=nav>
      <h1><a href="https://intmaker.com/">Int Maker</a></h1>
      <ul>
        
          <li><a href="https://intmaker.com/about/">About</a></li>
        
          <li><a href="https://intmaker.com/">Posts</a></li>
        
          <li><a href="https://intmaker.com/projects/">Projects</a></li>
        
      </ul>
    </section>


<section id=content>
  

  <h1> Inference of TensorFlow model without TensorFlow </h1>
  
    <div id=sub-header>
      March 2019 Â· 6 minute read
    </div>
  

  
  <div class="thumbnail-container">
      <a href="https://intmaker.com/2019/tensorflow-inference/">
          <img src="https://intmaker.com/img/tf-inference.jpg">
      </a>
  </div>
  

  <div class="entry-content">
    <p>Training of machine learning models is fun, but it&rsquo;s a useless waste of energy if you are not going to use them afterwards. Usage patterns can be very different. In some case you are creating Software-as-a-Service where you run inference on your or somebodies cloud and return customers only the results. In others - you need to have the model itself on consumer devices (e.g. mobile phones) and run inference there.</p>

<p>If you were training model using popular framework <a href="https://tensorflow.org">TensorFlow</a> you&rsquo;re covered in a sense that there are number of options you can stick to. To name just a few: inference with Tensorflow from Python, C++ inference with native library (TensorFlow is written in C++ after all), C++ inference with Tensorflow Lite (inference on mobile phones and/or Raspberry Pi) with bindings to Java&amp;co, TensorFlow Serving (production-quality C++ HTTP server for inference using TF native library).</p>

<p>When running on a server you can use whatever is more convenient so in this article I&rsquo;m mostly concerned about second use-case where you need to run inference on consumer devices.</p>

<p>Let&rsquo;s see which options do we have. Actually there&rsquo;re not so many of these: TensorFlow native library (C++), TensorFlow Lite native library (C++) and third-party solutions (like project <a href="https://github.com/snipsco/tract">Tract</a> from company Snips, written in Rust). Original TensorFlow library compiled for Release on my computer weights 30MB. Imagine you need to deploy it along with your 100MB model and you have 30% overhead which might be even worse if you have a smaller model. TensorFlow Lite supports a subset of operations you can use and it&rsquo;s compilation is fine-tuned for Android projects. Nevertheless it&rsquo;s quite possible to compile it for desktop and it&rsquo;s size in Release was about 3MB. This is already much better but the problem is that most probably you&rsquo;re not even using half of the features of TensorFlow Computation Graph that the library provides.</p>

<p>What is left for us is to write such inference on our own. It&rsquo;s not as hard as it sounds and definitely much more fun!</p>

<h2 id="step-1-export-from-python">Step 1. Export from Python</h2>

<p>Originally TensorFlow exports a model into a ProtoBuf-serialized (and optionally - compressed) file with all tensors and operations represented as Graph nodes and edges. If you&rsquo;re training a feedforward neural network, it is a linear model: next layer&rsquo;s input is an ouput of previous layer. For linear model&rsquo;s inference you do not need a graph: in the simplest cases you only need weighs and biases of each layer. So we can do exactly this: export all weights and biases from your TensorFlow model:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">loaded_graph = tf.Graph()

<span style="font-weight:bold">with</span> tf.Session(graph=loaded_graph) <span style="font-weight:bold">as</span> sess:
    loader = tf.train.import_meta_graph(meta_file_path)
    loader.restore(sess, path)
    all_layers = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)

    <span style="font-weight:bold">for</span> layer <span style="font-weight:bold">in</span> all_layers:
        <span style="font-weight:bold">print</span>(<span style="font-style:italic">&#39;Saving layer: {0}</span><span style="font-weight:bold;font-style:italic">\n</span><span style="font-style:italic">&#39;</span>.format(layer.name))
        array = layer.eval()

        <span style="font-style:italic"># filesystems do not like files with colons inside</span>
        layer_file = layer.name.replace(<span style="font-style:italic">&#39;:&#39;</span>, <span style="font-style:italic">&#39;_&#39;</span>) + <span style="font-style:italic">&#39;.npz&#39;</span>

        <span style="font-weight:bold">with</span> open(layer_file, <span style="font-style:italic">&#39;wb&#39;</span>) <span style="font-weight:bold">as</span> file:
            numpy.savez(file, array)</code></pre></div>
<p>This will convert tensors to NumPy arrays and use NumPy&rsquo;s feature to save NDarray as zip archive with an extension <code>.npz</code>.</p>

<h2 id="step-2-import-in-c">Step 2. Import in C++</h2>

<p>It&rsquo;s relatively easy to import these arrays to C++. You can use <a href="http://github.com/rogersce/cnpy">cnpy</a> library that you can just include as 1 header and 1 cpp file in your project. It allows you to read shape of the array and it&rsquo;s data.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp"><span style="">#include</span> <span style="">&lt;cnpy/cnpy.h&gt;</span><span style="">
</span><span style=""></span>
<span style="font-weight:bold">auto</span> array = cnpy::npz_load(array_path, <span style="font-style:italic">&#34;arr_0&#34;</span>);
<span style="font-style:italic">// std::vector like python list (3, 4, 5)
</span><span style="font-style:italic"></span><span style="font-weight:bold">auto</span> shape = array.shape;
<span style="font-style:italic">// pointer to 1D array of raw data
</span><span style="font-style:italic"></span><span style="font-weight:bold">auto</span> array_data = array.data&lt;T&gt;();
</code></pre></div>
<p>There&rsquo;re few important things to note here. One is that TensorFlow has weights matrix of dense layer transposed in memory. That&rsquo;s why you need to transpose it again if you plan to use it in more &ldquo;common&rdquo; way (of dot product with input vector).</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp"><span style="font-weight:bold">auto</span> input_flatten_size = shape[0];
<span style="font-weight:bold">auto</span> output_size = shape[1];

<span style="font-weight:bold">const</span> size_t size = input_flatten_size*output_size;
std::vector&lt;<span style="">float</span>&gt; transpose(size, 0.f);

<span style="">int</span> height = input_flatten_size, width = output_size;
<span style="font-style:italic">// `tf.dense` behaves by contracting last index of input tensor with first index of weights tensor
</span><span style="font-style:italic">// so saved tensor is actually a transpose matrix of shape (inputs_n, outputs_n, 1)
</span><span style="font-style:italic">// do reverse transpose in memory
</span><span style="font-style:italic"></span><span style="font-weight:bold">for</span> (<span style="">int</span> n = 0; n &lt; size; n++) {
    <span style="">int</span> i = n / height;
    <span style="">int</span> j = n % height;
    transpose[n] = array_data[width*j + i];
}
</code></pre></div>
<p>Another is that it&rsquo;s tricky to read convolution layer properly: there&rsquo;re too many indices for 1D array so you should be careful with them.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp"><span style="font-weight:bold">auto</span> index = 0;
<span style="font-weight:bold">for</span> (<span style="font-weight:bold">auto</span> height = 0; height &lt; filter_height; height++) {
    <span style="font-weight:bold">for</span> (<span style="font-weight:bold">auto</span> width = 0; width &lt; filter_width; width++) {
        <span style="font-weight:bold">for</span> (<span style="font-weight:bold">auto</span> depth = 0; depth &lt; input_depth; depth++) {
            <span style="font-weight:bold">for</span> (<span style="font-weight:bold">auto</span> filter = 0; filter &lt; filters_number; filter++) {
                weights_(filter, height, width, depth) = array_data[index++];
            }
        }
    }
}
</code></pre></div>
<h2 id="step-3-inference">Step 3. Inference</h2>

<p>It&rsquo;s relatively easy to implement a fully-connected layer as trivial operation with dot product of matrix and vector. A little bit more effort is required for convolution layer but <code>feedforward()</code> part is quite simple anyways. Recently I started an educational project that will help to understand how to do that. <a href="https://github.com/ribtoks/yannpp">yannpp library</a> - a small C++ only implementation of deep neural network (capable both of learning and inference). Code is relatively simple and heavily documented so it should be easy to start and use it or create a better library.</p>

<p>With yannpp you can precreate a model in C++ and it&rsquo;s ready for use. Here you can see an example for VGG-16 model:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp"><span style="font-weight:bold">using</span> cnn_t = convolution_layer_2d_t&lt;<span style="">float</span>&gt;;
<span style="font-weight:bold">using</span> fc_t = fully_connected_layer_t&lt;<span style="">float</span>&gt;;
<span style="font-weight:bold">using</span> pl_t = pooling_layer_t&lt;<span style="">float</span>&gt;;
<span style="font-weight:bold">using</span> m_t = layer_metadata_t;

layers_list layers = {
<span style="font-style:italic">/* ############## CONVOLUTION LAYER 1 ################# */</span>
  std::make_shared&lt;cnn_t&gt;(
    shape3d_t(56, 56, 3), <span style="font-style:italic">// input
</span><span style="font-style:italic"></span>    shape3d_t(3, 3, 3), <span style="font-style:italic">// filter
</span><span style="font-style:italic"></span>    64, <span style="font-style:italic">// filters count
</span><span style="font-style:italic"></span>    1, <span style="font-style:italic">// stride
</span><span style="font-style:italic"></span>    padding_type::same,
    relu_activator,
    m_t{ <span style="font-style:italic">&#34;conv1_1&#34;</span> }),
  std::make_shared&lt;cnn_t&gt;(
    shape3d_t(56, 56, 64), <span style="font-style:italic">// input
</span><span style="font-style:italic"></span>    shape3d_t(3, 3, 64), <span style="font-style:italic">// filter
</span><span style="font-style:italic"></span>    64, <span style="font-style:italic">// filters count
</span><span style="font-style:italic"></span>    1, <span style="font-style:italic">// stride
</span><span style="font-style:italic"></span>    padding_type::same,
    relu_activator,
    m_t{ <span style="font-style:italic">&#34;conv1_2&#34;</span> }),
  std::make_shared&lt;pl_t&gt;(
    2, <span style="font-style:italic">// window
</span><span style="font-style:italic"></span>    2, <span style="font-style:italic">// stride
</span><span style="font-style:italic"></span>    m_t{ <span style="font-style:italic">&#34;pool1&#34;</span> }),
<span style="font-style:italic">/* ############## CONVOLUTION LAYER 2 ################# */</span>
....
};

yannpp::network2_t&lt;<span style="">float</span>&gt; network(layers);
<span style="font-weight:bold">auto</span> input = read_image(testsDir + <span style="font-style:italic">&#34;/test_4.JPEG&#34;</span>);
<span style="font-weight:bold">auto</span> output = network.feedforward(input);
</code></pre></div>
<p>The most sensible way to use such library is to statically link it or just include all files in your project. Resulting overhead size of your executable will be only a couple of kilobytes even including <code>cnpy</code> library for reading <code>.npz</code> archives.</p>

<p><img src="https://intmaker.com/img/vgg16-inference.png" alt="Inference example" />
<em>Example of inference of VGG-16 model</em></p>

<p>Performance of this tiny inference engine is not that bad, although it can be heavily optimized by using vectorized operations for dot products.</p>

<h2 id="conclusion">Conclusion</h2>

<p>There are couple of options to run inference for TensorFlow-trained models. However, if you&rsquo;re after reducing size of your deployed application you would need to have something tailored right for you. The best you can have is custom inference code and as you can see from this post it is not so hard to achieve.</p>

<p><strong>Full code</strong> of VGG-16 export, load and inference can be found in this <a href="https://github.com/Vearol/Tensorflow-Model-Inference">awesome repository</a>.</p>

<p>I would be really happy to read in the comments how do you run inference on consumer devices.</p>
  </div>

  
  <div class="sharing-icons">
    Share:&nbsp;
    <a href="https://twitter.com/intent/tweet?text=&amp;url=" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
    <a href="https://www.facebook.com/sharer/sharer.php?u=&amp;title=" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
</div>

  <div style="margin-bottom: 20px;">
      <a class="bmc-button" target="_blank" href="https://www.buymeacoffee.com/TQOwEld1x"><img src="https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg" alt="Buy me a coffee"><span style="margin-left:5px">Buy me a coffee</span></a>

  </div>
  <div id=links>
    
      <a class="basic-alignment left" href="https://intmaker.com/2018/dependency-management/">&laquo; Dependency management in classes</a>
    
    
      <a class="basic-alignment left" href="https://intmaker.com/2019/how-to-return-to-flow/">How to return to the flow faster &raquo;</a>
    
  </div>

  
  <section class="disqus">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' +  null  + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>

  
  
</section>

  
</body>
</html>



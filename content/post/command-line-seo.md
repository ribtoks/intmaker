---
title: Command-line SEO alternative to Screaming Frog Spider
date: 2019-11-27T08:19:03-08:00
author: "Taras Kushnir"
image: seo.jpg
categories:
  - Programming
keywords:
  - seo
  - go
  - command
  - line
---

Did you ever wanted to crawl a list of URLs and figure out which of them are alive? Or what number of external links any of them has? If you did, most probably [Screaming Frog SEO Spider](https://www.screamingfrog.co.uk/seo-spider/) was one of the popular choices. Bad news is that free version has a limitation to 500 URLs and paid version costs lots of money (around $150 per year). And worst is that 80% of that crawling functionality can be done in pure Bash.

<!--more-->

However, pure Bash has also some limitations, so I decided to spend a day and create a better tool, written in Go. Meet [mink](https://github.com/ribtoks/mink), a command-line SEO spider. Mink can crawl a list of URLs and extract useful metrics like HTTP status code, indexibility, number of external and internal links and many many others.

## Usage

Here's a list of options:

```
Usage of mink:
  -d int
    	Maximum depth for crawling (default 1)
  -f string
    	Format of the output table|csv|tsv (default "table")
  -v	Write verbose logs
```

`mink` reads URLs from `STDIN` and writes reports to `STDOUT`. Report can be written in a form of a table, comma-separated values and tab-separated values.

Here are some examples of usage. In order to crawl the whole website, you can do this:

`echo "https://your-website.com" | mink -d 1000 -f csv > report.csv`

whereas to crawl a file with an URL per line, do this instead:

`cat urls.txt | mink -f csv > report.csv`

## Resource page link building

Probably most viable report format for `mink` is CSV since you can do fancy things like [resource page link building](https://www.youtube.com/watch?v=8f4YTubL6cM) in a Spreadsheet software like [Google Docs](https://docs.google.com/) or [LibreOffice](https://www.libreoffice.org/).

First you can go to Google and search `inurl:links.html fitness` (or any other keyword). Don't forget to change Search Settings in Google and use 100 results per page. 

![Google search settings](/img/google-search-settings.png)

From there you can use any visual scraping browser extension and grab urls using XPath `//div[@class="srg"]/div/div/div/div/a/@href`. Copy all of the urls to a text file `urls.txt` and run `cat urls.txt | mink -f csv > report.csv`.

Now go to Google Docs and run File -> Import -> Upload and select the CSV generated by `mink`. You will see something like this:

![Google Sheets report](/img/google-sheets-report.png)

From there you can add filters to the columns and filter out pages that are not Indexable or that are not returning HTTP 200. Then you're not interested in pages that don't have external links so you can set a filter to have minimum of 5 external links.

All this information is available in `mink` and is extremely fast to generate.

## Contibuting

[mink](https://github.com/ribtoks/mink) is an open-source project so you can make it better too! Feel free to send a pull request or reach out to me if you have any feature requests.

`mink` uses such great projects like [colly](https://github.com/gocolly/colly) for running the crawl, [tablewriter](github.com/olekukonko/tablewriter) for formatting and [goquery](github.com/PuerkitoBio/goquery) to query html.
